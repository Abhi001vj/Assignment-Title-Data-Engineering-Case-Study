# Assignment-Title-Data-Engineering-Case-Study
An assessment on how to handle programmatic advertising and manages multiple online advertising campaigns 
# Assignment Title: Data Engineering Case Study

Imagine you are a data engineer working for AdvertiseX, a digital advertising technology company. AdvertiseX specializes in programmatic advertising and manages multiple online advertising campaigns for its clients. The company handles vast amounts of data generated by ad impressions, clicks, conversions, and more. Your role as a data engineer is to address the following challenges:

# Data Sources and Formats:

## Ad Impressions:

Data Source: AdvertiseX serves digital ads to various online platforms and websites.
Data Format: Ad impressions data is generated in JSON format, containing information such as ad creative ID, user ID, timestamp, and the website where the ad was displayed.

## Clicks and Conversions:

Data Source: AdvertiseX tracks user interactions with ads, including clicks and conversions (e.g., sign-ups, purchases).
Data Format: Click and conversion data is logged in CSV format and includes event timestamps, user IDs, ad campaign IDs, and conversion type.
## Bid Requests:

### Data Source: AdvertiseX participates in real-time bidding (RTB) auctions to serve ads to users.
### Data Format: Bid request data is received in a semi-structured format, mostly in Avro, and includes user information, auction details, and ad targeting criteria.

## Case Study Solution
Detailed Data Engineering Solution for AdvertiseX
### Overview
This solution is designed to efficiently process and analyze ad impressions, clicks/conversions, and bid requests data to optimize AdvertiseX's programmatic advertising campaigns. The system handles JSON, CSV, and Avro data formats, ensuring high throughput, data integrity, and actionable insights for campaign performance.

1. Data Ingestion
**Objective**
Build a robust data ingestion system capable of handling real-time and batch data in various formats.

**Technologies**
- Apache Kafka for real-time data ingestion
- Apache NiFi or Apache Flume for data routing and preprocessing
- Apache Avro for handling schema-based binary data formats
**Architecture Diagram**
```
+-------------+    +-----------+    +------------+
| Data Source | -> | Apache    | -> | Apache     |
| (JSON/CSV/  |    | NiFi/Flume|    | Kafka      |
| Avro)       |    +-----------+    +------------+
+-------------+                      |     |
                                     v     v
                            +----------------------+
                            | Data Processing &    |
                            | Transformation Layer |
                            +----------------------+
```
#### Implementation Steps
**Setup Apache Kafka:**

- Deploy Kafka brokers.
- Create topics for ad impressions, clicks/conversions, and bid requests.
**Configure Apache NiFi:**

- Create data flow processors for each data source type.
- Transform and route data to the appropriate Kafka topics.

Key Considerations:
Schema Management: Implement schema registry for Avro data to manage versioning and compatibility.
Data Normalization: Preprocess data to normalize varying formats and structures, ensuring consistency before ingestion into the processing system.
Error Handling: Establish robust error handling mechanisms for malformed data, including retries, logging, and dead-letter queues.
2. Data Processing
**Objective**
- Standardize, validate, and enrich incoming data.
- Correlate ad impressions with clicks and conversions.
**Technologies**
- Apache Spark
Architecture Diagram
```
+------------+    +------------------+    +-----------------+
| Apache     | -> | Data Processing  | -> | Data Lake/      |
| Kafka      |    | (Apache Spark)   |    | Warehouse       |
+------------+    +------------------+    +-----------------+
                     |        ^                  |
                     |        |                  |
                     +--------+                  v
                 Data Validation &     Querying & Analysis
                 Transformation

```
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("AdDataProcessing").getOrCreate()

# Read from Kafka
impressions_df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "localhost:9092") \
  .option("subscribe", "ad-impressions-topic") \
  .load()

# Data Transformation (Example: Filtering Invalid Impressions)
valid_impressions_df = impressions_df.filter(col("user_id").isNotNull())

# Write to Data Lake/Warehouse
valid_impressions_df.writeStream \
  .format("parquet") \
  .option("checkpointLocation", "/path/to/checkpoint") \
  .option("path", "/path/to/data/lake") \
  .start()
```
**Cleaning**
Cleaning our data involves apply constraints to make it easier for our models to extract signal from the data.

- use domain expertise and EDA
- apply constraints via filters
- ensure data type consistency
- removing data points with certain or null column values

**Transformations**
Transforming the data involves feature encoding and engineering.
1. Scaling
- required for models where the scale of the input affects the processes
- learn constructs from train split and apply to other splits (local)
- don't  scale features features like categorical features
- standardization: rescale values to mean 0, std 1
2. Encoding
allows for representing data efficiently (maintains signal) and effectively (learns patterns, ex. one-hot vs embeddings)
- label: unique index for categorical value
- one-hot: representation as binary vector
- embeddings: dense representations capable of representing context
3. Extraction
- signal extraction from existing features
- combine existing features
- transfer learning: using a pretrained model as a feature extractor and finetuning on it's results
- autoencoders: learn to encode inputs for compressed knowledge representation
- principle component analysis (PCA): linear dimensionality reduction to project data in a lower dimensional space.

standardization: rescale values to mean 0, std 1
#### Feature Extraction
User Engagement Features: Extract metrics such as click-through rate (CTR), time spent on the ad, and interaction depth.
Ad Performance Features: Calculate ad impressions, clicks, conversion rates, and cost per acquisition (CPA).
Contextual Features: Include data about the user's device, location, time of day, and the content surrounding the ad impression.
**Common Metrics and KPIs**
- Click-Through Rate (CTR): The ratio of users who click on an ad to the number of total users who view the ad (impressions).
- Conversion Rate: The percentage of clicks that resulted in a conversion action (e.g., purchase, sign-up).
- Return on Advertising Spend (ROAS): The amount of revenue generated for every dollar spent on advertising.
- Cost Per Click (CPC) and Cost Per Thousand Impressions (CPM): Key cost metrics for campaign performance evaluation.
**Data Enrichment**
- Temporal Enrichment: Add features related to time, such as part of the day, weekday vs. weekend, and seasonality.
- Geographical Enrichment: Enhance data with geographical insights, such as region-specific performance and user demographics.
- Behavioral Segmentation: Classify users based on their interaction patterns and history to tailor future ad delivery.
#### Visualization and Reporting
**Objective:** Leverage data visualization and reporting tools to create actionable insights and intuitive dashboards for campaign performance monitoring.

**Visualization Techniques**
- Time Series Analysis: Visualize trends over time for key metrics like CTR, CPC, and conversion rates.
- Geographical Heatmaps: Display performance metrics across different regions to identify high and low-performing areas.
- Funnel Analysis: Create funnels to visualize the conversion path from impressions to clicks to conversions, identifying drop-off points.

#### Dashboard and Reporting Tools
- Apache Superset or Grafana for real-time analytics dashboards.
- Tableau or Power BI for in-depth analysis and interactive reporting.
- Integration with Apache Airflow for automated reporting workflows and alerts based on performance thresholds.
KPIs and Alerts
- Set up real-time alerts for significant changes in KPIs (e.g., sudden drops in CTR or spikes in CPC), allowing quick response to potential issues.
- Customizable dashboard widgets to track the performance of specific campaigns, ad groups, or creatives against their targets.

3. Data Storage and Query Performance
**Objective**
- Efficiently store processed data, enabling fast and flexible querying for analytics.
- Utilize a real-time NoSQL database for immediate data access and analysis.
**Technologies**
- Amazon Redshift
- Apache Hadoop HDFS + Hive
- Real-time NoSQL database (e.g., Apache Cassandra, MongoDB)
Architecture Diagram
```
+------------------+    +-------------------+
| Data Processing  | -> | Data Lake/Warehouse|
| (Apache Spark)   |    | (HDFS + Hive/      |
+------------------+    |  Amazon Redshift)  |
                         +-------------------+
                                   |
                                   v
                          Fast Query Performance
                                   |
                                   v
                         +---------+---------+
                         | Real-time NoSQL DB|
                         +-------------------+

```
Implementation Steps
Data Lake Storage:

Store processed data in HDFS using Parquet format.
Data Warehousing:

Use Amazon Redshift for analytical queries.
Real-time NoSQL Database:

Deploy a NoSQL database for real-time data querying and analysis.
Sample Hive Table Creation
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS ad_impressions (
  impression_id STRING,
  user_id STRING,
  ad_id STRING,
  timestamp TIMESTAMP
)
STORED AS PARQUET
LOCATION '/path/to/data/lake/ad_impressions/';
```
4. Error Handling and Monitoring
Objective
Develop a comprehensive monitoring system to detect data anomalies, drifts, discrepancies, or delays in real-time. Implement sophisticated alerting mechanisms to promptly address data quality issues, ensuring the effectiveness of ad campaigns is maintained.

**Technologies**
- Apache Airflow for workflow management and error handling.
- Prometheus for collecting metrics and monitoring system health.
- Grafana for visualizing metrics and setting up dashboards.
- Great Expectations for data validation and expectation suites.
- Alibi-Detect for drift detection and outlier identification.
**Advanced Monitoring Techniques**
1. Drift Detection
- Data Drift: Monitor the distribution of incoming data for significant changes compared to historical data. Utilize statistical tests like Kolmogorov-Smirnov (KS) for continuous features and Chi-squared tests for categorical features to detect drift.

- Concept Drift: Detect shifts in the relationship between input data and output predictions. Implement adaptive models or triggers for retraining when concept drift is detected.

- Model Performance Drift: Track sliding window performance metrics (e.g., accuracy, precision, recall) and compare them against historical performance to identify degradation.

2. Anomaly and Outlier Detection
Implement unsupervised methods to identify anomalous data points that deviate significantly from the norm. Techniques such as Isolation Forests, DBSCAN, or Autoencoder-based methods can be employed to flag unusual data for further inspection.
**Expectation Suites**
Use Great Expectations to define and validate data quality requirements. Expectations can include checks for missing values, data type validations, and range checks to ensure incoming data meets predefined standards.
**Real-time Alerting and Incident Management**
Define alerting thresholds based on the severity and impact of detected issues. Utilize tools like Grafana for dashboard alerts or integrate with incident management platforms like PagerDuty for real-time notifications.

Implement a hierarchy of alerts to differentiate between critical issues that require immediate attention and warnings that can be reviewed in a regular maintenance window.

Architecture Diagram
```
+-------------------------+    +-----------------+
| Data Processing &       | -> | Error Handling  |
| Transformation Layer    |    | & Monitoring    |
+-------------------------+    +-----------------+
                                    |       |
                                    v       v
                              +------+    +------+
                              | Alert|    |Dashboard|
                              +------+    +------+
```
Sample implementing Alerting with Apache Airflow
```python
Copy code
from airflow import DAG
from airflow.operators.email_operator import EmailOperator
from datetime import datetime, timedelta

# Define default arguments for the DAG
default_args = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'email': ['alert@advertisex.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Define the DAG
dag = DAG(
    'ad_data_pipeline_monitoring',
    default_args=default_args,
    description='Monitoring Ad Data Pipeline',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2024, 1, 1),
)

# Define a task for sending alert emails
task_send_alert = EmailOperator(
    task_id='send_alert_email',
    to=['alert@advertisex.com'],
    subject='Ad Data Pipeline Alert',
    html_content='''<h3>Alert: A potential issue has been detected in the ad data pipeline.</h3>''',
    dag=dag,
)
```






