# Assignment-Title-Data-Engineering-Case-Study
An assessment on how to handle programmatic advertising and manages multiple online advertising campaigns 
Assignment Title: Data Engineering Case Study

Imagine you are a data engineer working for AdvertiseX, a digital advertising technology company. AdvertiseX specializes in programmatic advertising and manages multiple online advertising campaigns for its clients. The company handles vast amounts of data generated by ad impressions, clicks, conversions, and more. Your role as a data engineer is to address the following challenges:

Data Sources and Formats:

Ad Impressions:

Data Source: AdvertiseX serves digital ads to various online platforms and websites.
Data Format: Ad impressions data is generated in JSON format, containing information such as ad creative ID, user ID, timestamp, and the website where the ad was displayed.
Clicks and Conversions:

Data Source: AdvertiseX tracks user interactions with ads, including clicks and conversions (e.g., sign-ups, purchases).
Data Format: Click and conversion data is logged in CSV format and includes event timestamps, user IDs, ad campaign IDs, and conversion type.
Bid Requests:

Data Source: AdvertiseX participates in real-time bidding (RTB) auctions to serve ads to users.
Data Format: Bid request data is received in a semi-structured format, mostly in Avro, and includes user information, auction details, and ad targeting criteria.
Case Study Solution
Detailed Data Engineering Solution for AdvertiseX
Overview
This solution is designed to efficiently process and analyze ad impressions, clicks/conversions, and bid requests data to optimize AdvertiseX's programmatic advertising campaigns. The system handles JSON, CSV, and Avro data formats, ensuring high throughput, data integrity, and actionable insights for campaign performance.

1. Data Ingestion
Objective
Build a robust data ingestion system capable of handling real-time and batch data in various formats.

Technologies
Apache Kafka
Apache NiFi
Architecture Diagram
```
+-------------+    +-----------+    +------------+
| Data Source | -> | Apache    | -> | Apache     |
| (JSON/CSV/  |    | NiFi/Flume|    | Kafka      |
| Avro)       |    +-----------+    +------------+
+-------------+                      |     |
                                     v     v
                            +----------------------+
                            | Data Processing &    |
                            | Transformation Layer |
                            +----------------------+
```
Implementation Steps
Setup Apache Kafka:

Deploy Kafka brokers.
Create topics for ad impressions, clicks/conversions, and bid requests.
Configure Apache NiFi:

Create data flow processors for each data source type.
Transform and route data to the appropriate Kafka topics.
Sample Configuration
NiFi Processor for JSON:
```xml
<processor>
  <id>GetFile-JSON</id>
  <type>GetFile</type>
  <properties>
    <property name="Input Directory" value="/path/to/json/files" />
    <property name="File Filter" value=".*\.json" />
  </properties>
</processor>

<processor>
  <id>PublishKafka-JSON</id>
  <type>PublishKafka</type>
  <properties>
    <property name="Kafka Brokers" value="kafka-broker:9092" />
    <property name="Topic Name" value="ad-impressions-topic" />
  </properties>
</processor>
```
2. Data Processing
Objective
Standardize, validate, and enrich incoming data.
Correlate ad impressions with clicks and conversions.
Technologies
Apache Spark
Architecture Diagram
```
+------------+    +------------------+    +-----------------+
| Apache     | -> | Data Processing  | -> | Data Lake/      |
| Kafka      |    | (Apache Spark)   |    | Warehouse       |
+------------+    +------------------+    +-----------------+
                     |        ^                  |
                     |        |                  |
                     +--------+                  v
                 Data Validation &     Querying & Analysis
                 Transformation

```
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("AdDataProcessing").getOrCreate()

# Read from Kafka
impressions_df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "localhost:9092") \
  .option("subscribe", "ad-impressions-topic") \
  .load()

# Data Transformation (Example: Filtering Invalid Impressions)
valid_impressions_df = impressions_df.filter(col("user_id").isNotNull())

# Write to Data Lake/Warehouse
valid_impressions_df.writeStream \
  .format("parquet") \
  .option("checkpointLocation", "/path/to/checkpoint") \
  .option("path", "/path/to/data/lake") \
  .start()
```
3. Data Storage and Query Performance
Objective
Efficiently store processed data, enabling fast and flexible querying for analytics.
Utilize a real-time NoSQL database for immediate data access and analysis.
Technologies
Amazon Redshift
Apache Hadoop HDFS + Hive
Real-time NoSQL database (e.g., Apache Cassandra, MongoDB)
Architecture Diagram
```
+------------------+    +-------------------+
| Data Processing  | -> | Data Lake/Warehouse|
| (Apache Spark)   |    | (HDFS + Hive/      |
+------------------+    |  Amazon Redshift)  |
                         +-------------------+
                                   |
                                   v
                          Fast Query Performance
                                   |
                                   v
                         +---------+---------+
                         | Real-time NoSQL DB|
                         +-------------------+

```
Implementation Steps
Data Lake Storage:

Store processed data in HDFS using Parquet format.
Data Warehousing:

Use Amazon Redshift for analytical queries.
Real-time NoSQL Database:

Deploy a NoSQL database for real-time data querying and analysis.
Sample Hive Table Creation
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS ad_impressions (
  impression_id STRING,
  user_id STRING,
  ad_id STRING,
  timestamp TIMESTAMP
)
STORED AS PARQUET
LOCATION '/path/to/data/lake/ad_impressions/';
```
4. Error Handling and Monitoring
Objective
Monitor the data pipeline for any anomalies or failures, ensuring data quality and timely processing.

Technologies
Apache Airflow
Prometheus
Grafana
Architecture Diagram
```
+-------------------------+    +-----------------+
| Data Processing &       | -> | Error Handling  |
| Transformation Layer    |    | & Monitoring    |
+-------------------------+    +-----------------+
                                    |       |
                                    v       v
                              +------+    +------+
                              | Alert|    |Dashboard|
                              +------+    +------+
Sample Airflow Alert Configuration
```
```python
from airflow import DAG
from airflow.operators.email_operator import EmailOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'email': ['alert@advertisex.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'ad_data_pipeline',
    default_args=default_args,
    description='Ad Data Pipeline with Error Alerts',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2024, 1, 1),
)

task_send_alert = EmailOperator(
    task_id='send_alert_email',
    to=['alert@advertisex.com'],
    subject='Ad Data Pipeline Failure Alert',
    html_content='''<h3>Ad Data Pipeline has encountered a failure.</h3>''',
    dag=dag,
)
```







